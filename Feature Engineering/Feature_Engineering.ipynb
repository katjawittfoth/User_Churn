{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T06:57:55.812017Z",
     "start_time": "2019-02-26T06:57:54.230958Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T07:00:14.596389Z",
     "start_time": "2019-02-26T06:57:55.814582Z"
    }
   },
   "outputs": [],
   "source": [
    "events = pd.read_csv('events.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T02:57:43.642659Z",
     "start_time": "2019-02-27T02:57:11.270169Z"
    }
   },
   "outputs": [],
   "source": [
    "sessions = pd.read_csv('sessions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T07:38:03.790756Z",
     "start_time": "2019-02-26T07:29:37.737516Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert timestamp into day of year\n",
    "def timeconvert(ts):\n",
    "    ts = int(ts)/1000\n",
    "    return (datetime.utcfromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "events['time'] = events['event_timestamp'].apply(timeconvert)\n",
    "events['time'] = pd.to_datetime(events['time']) \n",
    "events['day_current'] = events['time'].dt.dayofyear "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T07:38:31.985452Z",
     "start_time": "2019-02-26T07:38:03.798861Z"
    }
   },
   "outputs": [],
   "source": [
    "sessions['time'] = sessions['start_timestamp'].apply(timeconvert)\n",
    "sessions['time'] = pd.to_datetime(sessions['time']) \n",
    "sessions['day_current'] = sessions['time'].dt.dayofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T07:39:08.721962Z",
     "start_time": "2019-02-26T07:38:31.987410Z"
    }
   },
   "outputs": [],
   "source": [
    "# split the data into train(before Dec) and test(whole dataset)\n",
    "events_train = events.loc[events.day_current <= 334]\n",
    "sessions_train = sessions.loc[sessions.day_current <= 334]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Feature Engineering\n",
    "### 2.1 Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T07:39:29.759082Z",
     "start_time": "2019-02-26T07:39:08.736245Z"
    }
   },
   "outputs": [],
   "source": [
    "# purchase in the past 20 days\n",
    "# train set\n",
    "purchase_before_train = events_train.loc[events_train.event == \"8\"]\n",
    "purchase_before_train = purchase_before_train.loc[purchase_before_train.day_current > 314]\n",
    "purchase_before_train = purchase_before_train.groupby(\"user_id_hash\")['event'].count().to_frame()\n",
    "\n",
    "# test set\n",
    "purchase_before = events.loc[events.event == \"8\"]\n",
    "purchase_before = purchase_before.loc[purchase_before.day_current > 328]\n",
    "purchase_before = purchase_before.groupby(\"user_id_hash\")['event'].count().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T07:39:41.290383Z",
     "start_time": "2019-02-26T07:39:29.764289Z"
    }
   },
   "outputs": [],
   "source": [
    "# total purchase value in the past\n",
    "value_purchase_train = events_train.loc[events_train.event == \"8\"].groupby('user_id_hash')['event_value'].sum().to_frame()\n",
    "value_purchase = events.loc[events.event == \"8\"].groupby('user_id_hash')['event_value'].sum().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T07:39:42.181167Z",
     "start_time": "2019-02-26T07:39:41.293496Z"
    }
   },
   "outputs": [],
   "source": [
    "# load label.csv (run label_extract.py first)\n",
    "label = pd.read_csv('label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T02:58:08.201684Z",
     "start_time": "2019-02-27T02:58:03.862101Z"
    }
   },
   "outputs": [],
   "source": [
    "# merge sessions with label\n",
    "sessions = pd.merge(sessions, label, on ='user_id_hash', how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T02:59:32.010236Z",
     "start_time": "2019-02-27T02:58:09.044682Z"
    }
   },
   "outputs": [],
   "source": [
    "events_train = pd.merge(events_train, label, on ='user_id_hash', how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T02:59:57.733160Z",
     "start_time": "2019-02-27T02:59:37.360473Z"
    }
   },
   "outputs": [],
   "source": [
    "# purchase percent of each country\n",
    "user_per_country = sessions.groupby(\"country\")['user_id_hash'].nunique()\n",
    "purchase_per_country = sessions.loc[sessions.user_purchase_binary_14_days == 1].groupby(\"country\")['user_id_hash'].nunique()\n",
    "pct_country = purchase_per_country/user_per_country\n",
    "pct_country = pct_country.to_frame()\n",
    "pct_country.columns = ['pct_country']\n",
    "pct_country = pct_country.fillna(0)\n",
    "sessions = pd.merge(sessions, pct_country, on ='country', how = \"left\")\n",
    "\n",
    "pct_country = sessions.groupby(\"user_id_hash\")['pct_country'].max().to_frame()\n",
    "pct_country = pct_country.reset_index()\n",
    "pct_country.columns = ['user_id_hash','pct_country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T03:00:24.682783Z",
     "start_time": "2019-02-27T03:00:08.419582Z"
    }
   },
   "outputs": [],
   "source": [
    "# purchase percent of each city\n",
    "user_per_city = sessions.groupby(\"city\")['user_id_hash'].nunique()\n",
    "purchase_per_city = sessions.loc[sessions.user_purchase_binary_14_days == 1].groupby(\"city\")['user_id_hash'].nunique()\n",
    "pct_city = purchase_per_city/user_per_city\n",
    "pct_city= pct_city.to_frame()\n",
    "pct_city.columns = ['pct_city']\n",
    "pct_city = pct_city.fillna(0)\n",
    "sessions = pd.merge(sessions, pct_city, on ='city', how = \"left\")\n",
    "\n",
    "pct_city = sessions.groupby(\"user_id_hash\")['pct_city'].max().to_frame()\n",
    "pct_city = pct_city.reset_index()\n",
    "pct_city.columns = ['user_id_hash','pct_city']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T07:41:09.646225Z",
     "start_time": "2019-02-26T07:41:05.709916Z"
    }
   },
   "outputs": [],
   "source": [
    "# mean sessions_duration\n",
    "sessions_duration_train = sessions_train.groupby('user_id_hash')['previous_sessions_duration'].mean().to_frame()\n",
    "sessions_duration = sessions.groupby('user_id_hash')['previous_sessions_duration'].mean().to_frame()\n",
    "# normalize\n",
    "sessions_duration_train = ((sessions_duration_train.previous_sessions_duration \n",
    "                            - sessions_duration_train.previous_sessions_duration.min())\n",
    "                           /(sessions_duration_train.previous_sessions_duration.max() \n",
    "                             - sessions_duration_train.previous_sessions_duration.min())).to_frame()\n",
    "sessions_duration = ((sessions_duration.previous_sessions_duration - sessions_duration.previous_sessions_duration.min())/(sessions_duration.previous_sessions_duration.max() - sessions_duration.previous_sessions_duration.min())).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T07:42:39.609934Z",
     "start_time": "2019-02-26T07:41:09.648721Z"
    }
   },
   "outputs": [],
   "source": [
    "# Number of unique sessions of each user\n",
    "session_unique_train = events_train.groupby(\"user_id_hash\")[\"session_id\"].nunique().to_frame()\n",
    "session_unique = events.groupby(\"user_id_hash\")[\"session_id\"].nunique().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T14:41:22.103538Z",
     "start_time": "2019-02-26T14:32:29.460509Z"
    }
   },
   "outputs": [],
   "source": [
    "# total number of each attribute of each user\n",
    "attributes = pd.read_csv('attributes.csv',error_bad_lines=False)\n",
    "attribute = attributes.groupby(['user_id_hash','attribute'])['session_id'].count()\n",
    "attribute = attribute.to_frame()\n",
    "attribute=attribute.reset_index()\n",
    "attribute = attribute.pivot(index='user_id_hash',columns='attribute',values='session_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T14:43:10.502952Z",
     "start_time": "2019-02-26T14:43:10.407648Z"
    }
   },
   "outputs": [],
   "source": [
    "attribute.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "counts for most attributes are the same, Thus, we only take attribute 0, 66, 67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T14:43:13.457816Z",
     "start_time": "2019-02-26T14:43:13.433316Z"
    }
   },
   "outputs": [],
   "source": [
    "attribute = attribute[[0,66,67]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Merge features together with label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T03:03:42.475604Z",
     "start_time": "2019-02-27T03:03:37.996724Z"
    }
   },
   "outputs": [],
   "source": [
    "#label_f= pd.merge(label, session_pct, on = \"user_id_hash\", how = \"left\")\n",
    "feature_train = pd.merge(label, session_unique_train, on = \"user_id_hash\", how = \"left\")\n",
    "feature_train = pd.merge(feature_train, purchase_before_train, on = \"user_id_hash\", how = \"left\")\n",
    "feature_train = pd.merge(feature_train,value_purchase_train, on = \"user_id_hash\", how = \"left\")\n",
    "feature_train = pd.merge(feature_train,sessions_duration_train, on = \"user_id_hash\", how = \"left\")\n",
    "feature_train = pd.merge(feature_train,pct_country, on = \"user_id_hash\", how = \"left\")\n",
    "feature_train = pd.merge(feature_train,pct_city, on = \"user_id_hash\", how = \"left\")\n",
    "feature_train = pd.merge(feature_train,attribute,on=\"user_id_hash\",how = \"left\")\n",
    "feature_train = feature_train.fillna(0)\n",
    "feature_train.columns = [\"user_id_hash\", \"user_purchase_binary_7_days\", \"user_purchase_binary_14_days\",\n",
    "                  \"unique_session\",\"purchase_before\",\"value_purchase\",\"sessions_duration\",'pct_country','pct_city','0','66','67']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T14:43:21.978658Z",
     "start_time": "2019-02-26T14:43:21.490163Z"
    }
   },
   "outputs": [],
   "source": [
    "# load example submission \n",
    "res = pd.read_csv(\"sample_submission_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T03:03:45.336716Z",
     "start_time": "2019-02-27T03:03:42.478206Z"
    }
   },
   "outputs": [],
   "source": [
    "#label_f= pd.merge(label, session_pct, on = \"user_id_hash\", how = \"left\")\n",
    "feature_test = pd.merge(res, session_unique_train, on = \"user_id_hash\", how = \"left\")\n",
    "feature_test = pd.merge(feature_test, purchase_before_train, on = \"user_id_hash\", how = \"left\")\n",
    "feature_test = pd.merge(feature_test,value_purchase_train, on = \"user_id_hash\", how = \"left\")\n",
    "feature_test = pd.merge(feature_test,sessions_duration_train, on = \"user_id_hash\", how = \"left\")\n",
    "feature_test = pd.merge(feature_test,pct_country, on = \"user_id_hash\", how = \"left\")\n",
    "feature_test = pd.merge(feature_test,pct_city, on = \"user_id_hash\", how = \"left\")\n",
    "feature_test = pd.merge(feature_test,attribute,on=\"user_id_hash\",how = \"left\")\n",
    "feature_test = feature_test.fillna(0)\n",
    "feature_test.columns = [\"user_id_hash\", \"user_purchase_binary_7_days\", \"user_purchase_binary_14_days\",\n",
    "                  \"unique_session\",\"purchase_before\",\"value_purchase\",\"sessions_duration\",'pct_country','pct_city','0','66','67']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T03:03:46.728795Z",
     "start_time": "2019-02-27T03:03:46.712744Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T05:31:01.418222Z",
     "start_time": "2019-02-27T05:30:50.862700Z"
    }
   },
   "outputs": [],
   "source": [
    "# load the features to disk\n",
    "feature_train.to_csv(\"feature_train_org.csv\",index=False)\n",
    "feature_test.to_csv(\"feature_test_org.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
